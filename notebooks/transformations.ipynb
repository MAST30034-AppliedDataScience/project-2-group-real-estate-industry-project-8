{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/curated/NO_DOMAIN_DATASET.csv\")\n",
    "coast = pd.read_csv(\"../data/curated/vic_beach_proximity.csv\")\n",
    "schools = pd.read_csv(\"../data/curated/schools_by_SAL.csv\")\n",
    "tourism = pd.read_csv(\"../data/curated/victoria_gdp_tourism.csv\")\n",
    "airbnb = pd.read_csv(\"../data/curated/airbnb_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7358, 63)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data\n",
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get years from 2016-2023\n",
    "\n",
    "\n",
    "data_train = pd.merge(data_train, tourism, on='year', how='inner')\n",
    "data_train = pd.merge(data_train, airbnb, left_on='SAL_CODE', right_on='SAL_CODE21', how='left')\n",
    "data_train = pd.merge(data_train, schools, on='SAL_CODE', how='inner')\n",
    "\n",
    "data_train.rename(columns={'counts':'airbnb_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5551, 65)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = pd.read_csv('../data/landing/filtered_sal_codes.csv')\n",
    "\n",
    "to_exclude = list(to_exclude['SAL_CODE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also exclude any airport\n",
    "data_train = data_train[~data_train['SAL_suburb'].str.contains('Airport')]\n",
    "\n",
    "data_train = data_train[~data_train['SAL_CODE'].isin(to_exclude)]\n",
    "\n",
    "data_train = data_train[~data_train['SAL_suburb'].str.contains(\"Ravenhall\")]\n",
    "\n",
    "# Get counts per year\n",
    "year_counts = data_train['year'].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['gdp_impact', 'airbnb_count'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_to_CBD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_to_CBD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_to_station\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_to_station\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA Crimes against the person\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage_household_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage_num_psns_per_bedroom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealthcare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustrial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic_transport\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecreation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidential\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshopping\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minflation\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAL_CODE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_price\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproximity_to_beach\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgdp_impact\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairbnb_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ADS/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ADS/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ADS/lib/python3.11/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['gdp_impact', 'airbnb_count'] not in index\""
     ]
    }
   ],
   "source": [
    "cols = ['distance_to_CBD', 'time_to_CBD','distance_to_station', 'time_to_station', \n",
    "        'year', 'A Crimes against the person',\n",
    "       'Average_household_size', 'Average_num_psns_per_bedroom',\n",
    "       'B Property and deception offences', 'C Drug offences',\n",
    "       'D Public order and security offences', 'E Justice procedures offences',\n",
    "       'F Other offences', 'Median_age_persons', 'Median_tot_fam_inc_weekly',\n",
    "       'Median_tot_hhd_inc_weekly', 'Median_tot_prsnl_inc_weekly', 'Tot_P_P',\n",
    "        'average_quarterly_count', 'average_weekly_rent',\n",
    "        'commercial', 'education', 'food_establishments',\n",
    "       'healthcare', 'industrial', 'public_transport', 'recreation',\n",
    "       'residential', 'shopping', 'inflation',\"SAL_CODE\", \"median_score\", 'average_price',\"proximity_to_beach\", 'gdp_impact', 'airbnb_count']\n",
    "\n",
    "data_train = data_train[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data to be divided by area\n",
    "div_area = ['commercial', 'education', 'food_establishments','healthcare', 'industrial', 'public_transport', 'recreation','residential', 'shopping']\n",
    "\n",
    "# get SAL codes\n",
    "sal_boundaries = gpd.read_file('../data/landing/SAL_data/SAL_2021_AUST_GDA2020.shp')\n",
    "sal_boundaries = sal_boundaries[['SAL_CODE21','AREASQKM21']]\n",
    "\n",
    "# Filter out non-numeric SAL_CODE21 values\n",
    "sal_boundaries = sal_boundaries[sal_boundaries['SAL_CODE21'].apply(lambda x: str(x).isdigit())]\n",
    "sal_boundaries['SAL_CODE21'] = sal_boundaries['SAL_CODE21'].astype(int)\n",
    "\n",
    "data_train = pd.merge(data_train, sal_boundaries, left_on='SAL_CODE', right_on='SAL_CODE21', how='inner')\n",
    "\n",
    "# divide by area\n",
    "for col in div_area:\n",
    "    data_train[col + ' density'] = data_train[col] / data_train['AREASQKM21']\n",
    "    data_train.drop(columns=[col], inplace=True)\n",
    "\n",
    "data_train['pop density'] = data_train['Tot_P_P'] / data_train['AREASQKM21']\n",
    "\n",
    "data_train = data_train.drop(columns=['AREASQKM21','SAL_CODE21'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_interest = list(range(2016, 2030))\n",
    "interest_rates = [\n",
    "    1.75,  # 2016\n",
    "    1.50,  # 2017\n",
    "    1.50,  # 2018\n",
    "    1.50,  # 2019\n",
    "    0.25,  # 2020 (COVID-19 impact, very low rates)\n",
    "    0.10,  # 2021 (near-zero rates)\n",
    "    0.35,  # 2022 (start of increase)\n",
    "    3.10,  # 2023 (rise due to inflation concerns)\n",
    "    4.00,  # 2024 (projection)\n",
    "    3.75,  # 2025 (projection)\n",
    "    3.50,  # 2026 (projection)\n",
    "    3.25,  # 2027 (projection)\n",
    "    3.00,  # 2028 (projection)\n",
    "    2.75   # 2029 (projection)\n",
    "]\n",
    "\n",
    "# Creating a DataFrame\n",
    "interest_rate_table = pd.DataFrame({\n",
    "    \"Year\": years_interest,\n",
    "    \"Projected Interest Rate (%)\": interest_rates\n",
    "})\n",
    "\n",
    "# create df with years and gdp growth rate\n",
    "gdp_df = pd.DataFrame({\n",
    "    'year': range(2016, 2030),\n",
    "    'interest_rate': interest_rates\n",
    "})\n",
    "\n",
    "# Merge the GDP growth rate data with the main dataframe on the 'year' column\n",
    "data_train = pd.merge(data_train, gdp_df, on='year', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is called df\n",
    "# First, ensure that your DataFrame is sorted by 'year'\n",
    "df = data_train\n",
    "df = df.sort_values('year')\n",
    "\n",
    "# Convert 'inflation' rates from percentages to decimals\n",
    "df['inflation_decimal'] = df['inflation'] / 100\n",
    "\n",
    "# Create a DataFrame containing all years from 2016 to the maximum year in your data\n",
    "years = pd.DataFrame({'year': range(2016, df['year'].max() + 1)})\n",
    "\n",
    "# Merge with the inflation data to ensure all years are included\n",
    "inflation_data = pd.merge(years, df[['year', 'inflation_decimal']].drop_duplicates(), on='year', how='left')\n",
    "\n",
    "# Forward-fill any missing inflation rates (if any years are missing)\n",
    "inflation_data['inflation_decimal'] = inflation_data['inflation_decimal'].fillna(method='ffill')\n",
    "\n",
    "# Calculate the cumulative inflation factor from 2016 to each year\n",
    "inflation_data['cumulative_inflation_factor'] = (1 + inflation_data['inflation_decimal']).cumprod()\n",
    "\n",
    "# Create a dictionary to map years to cumulative inflation factors\n",
    "cumulative_inflation_dict = inflation_data.set_index('year')['cumulative_inflation_factor'].to_dict()\n",
    "\n",
    "# Map the cumulative inflation factors back to the original DataFrame\n",
    "df['cumulative_inflation_factor'] = df['year'].map(cumulative_inflation_dict)\n",
    "\n",
    "\n",
    "# Update the 'inflation' column to reflect the cumulative effect from 2016\n",
    "df['inflation'] = df['cumulative_inflation_factor']\n",
    "\n",
    "# Drop the temporary columns if you no longer need them\n",
    "df = df.drop(columns=['inflation_decimal', 'cumulative_inflation_factor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>inflation_decimal</th>\n",
       "      <th>cumulative_inflation_factor</th>\n",
       "      <th>interest_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>1.013000</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.034273</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.053924</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.013</td>\n",
       "      <td>1.067625</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.091113</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.103115</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.159374</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.070</td>\n",
       "      <td>1.240530</td>\n",
       "      <td>3.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.285189</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.323745</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.359486</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.396192</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2028</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.432493</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  inflation_decimal  cumulative_inflation_factor  interest_rate\n",
       "0   2016              0.013                     1.013000           1.75\n",
       "1   2017              0.021                     1.034273           1.50\n",
       "2   2018              0.019                     1.053924           1.50\n",
       "3   2019              0.013                     1.067625           1.50\n",
       "4   2020              0.022                     1.091113           0.25\n",
       "5   2021              0.011                     1.103115           0.10\n",
       "6   2022              0.051                     1.159374           0.35\n",
       "7   2023              0.070                     1.240530           3.10\n",
       "8   2024              0.036                     1.285189           4.00\n",
       "9   2025              0.030                     1.323745           3.75\n",
       "10  2026              0.027                     1.359486           3.50\n",
       "11  2027              0.027                     1.396192           3.25\n",
       "12  2028              0.026                     1.432493           3.00"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine inflation and gdp growth rate per year:\n",
    "# Merge the GDP growth rate data with the main dataframe on the 'year' column\n",
    "inflation_data = pd.merge(inflation_data, gdp_df, on='year', how='left')\n",
    "inflation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inflation affected columns\n",
    "inflation_affected_cols = ['average_weekly_rent', 'Median_tot_fam_inc_weekly', 'Median_tot_hhd_inc_weekly', 'Median_tot_prsnl_inc_weekly', 'gdp_impact']\n",
    "\n",
    "# Apply inflation adjustment to the affected columns\n",
    "for col in inflation_affected_cols:\n",
    "    df[col + '/inflation'] = df[col] / df['inflation']\n",
    "\n",
    "    df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['gdp_cbd/inflation/beach'] = np.log(df['gdp_impact/inflation']) * df['proximity_to_beach']\n",
    "df['gdp_cbd/inflation/cbd'] = np.log(df['gdp_impact/inflation']) * df['distance_to_CBD']\n",
    "df['gpd/inflation/airbnb'] = np.log(df['gdp_impact/inflation']) * df['airbnb_count']\n",
    "\n",
    "# drop the original columns\n",
    "df.drop(columns=['gdp_impact/inflation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['inflation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by average_household_size to get 'per person' rent\n",
    "df['average_weekly_rent/inflation/household_size'] = df['average_weekly_rent/inflation'] / (df['Average_household_size'] + 0.01)\n",
    "df.drop(columns=['average_weekly_rent/inflation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crime rate per person\n",
    "crime_cols = ['A Crimes against the person', 'B Property and deception offences', 'C Drug offences', 'D Public order and security offences', 'E Justice procedures offences', 'F Other offences']\n",
    "\n",
    "for col in crime_cols:\n",
    "    df[col + '/per_person'] = df[col] / (df['Tot_P_P'] + 0.01)\n",
    "\n",
    "    df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log data for every column but average weekly rent\n",
    "for col in df.columns:\n",
    "    if col not in ['average_weekly_rent/inflation/household_size', 'SAL_CODE', 'year', 'interest_rate']:\n",
    "        df[col] = df[col].apply(lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/curated/Processed Data Final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
